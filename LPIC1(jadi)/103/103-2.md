# LPIC-1: Session 9 - Process Text Streams Using Filters

## üìå Objectives
- Understand and manipulate **text streams** in Linux
- Use text processing commands such as `cat`, `cut`, `sort`, `sed`, `tr`, `wc`, and more
- Apply **filters** to modify and analyze text output
- Understand **standard input (stdin)** and **output (stdout)** streams

---

## üìÇ Understanding Text Streams
In Linux, most data (logs, configurations, etc.) is stored as **text**. Processing this text involves:
- **Reading input** (from a file, keyboard, or another command)
- **Modifying the text** using commands (filters)
- **Writing output** (to a file, display, or another command)

### **Standard Streams**
| Stream | Description |
|--------|-------------|
| `stdin`  | Standard input (default: keyboard) |
| `stdout` | Standard output (default: screen) |
| `stderr` | Standard error output (default: screen) |

For example, you can use **pipes (`|`)** to send the output of one command as input to another:
```bash
ls -l | less  # View directory listing page by page
```

---

## üîç Viewing Commands
### **`cat` - Display File Contents**
```bash
cat file.txt  # Show contents of file
cat file1 file2 > merged.txt  # Merge files into one
cat -n file.txt  # Show line numbers
```

### **`bzcat`, `xzcat`, `zcat` - View Compressed Files**
```bash
bzcat file.bz2
xzcat file.xz
zcat file.gz
```

### **`less` - View Large Files Efficiently**
```bash
less file.txt  # Open file with navigation
q  # Exit
/PageUp, PageDown  # Scroll pages
/foo  # Search for 'foo'
```

### **`od` - View File in Different Formats**
```bash
od -c file.txt  # Show characters
od -A x -t x1 file.txt  # Show hex dump
```

---

## ‚úÇÔ∏è Cutting & Splitting Files
### **`cut` - Extract Columns of Text**
```bash
cut -f1 -d ' ' file.txt  # Extract first field (column), space-separated
```

### **`split` - Divide a Large File**
```bash
split -l 100 file.txt  # Split into 100-line chunks
split -b 10M file.txt  # Split into 10MB chunks
```

### **`head` & `tail` - Show First/Last Lines**
```bash
head -n 5 file.txt  # Show first 5 lines

# Follow a log file in real-time
tail -f /var/log/syslog
```

---

## üîÑ Sorting & Uniqueness
### **`sort` - Sort Lines Alphabetically or Numerically**
```bash
sort file.txt  # Sort alphabetically
sort -n file.txt  # Sort numerically
sort -r file.txt  # Reverse order
```

### **`uniq` - Remove Duplicate Lines**
```bash
sort file.txt | uniq  # Remove duplicates
sort file.txt | uniq -c  # Show count of occurrences
```

### **`paste` - Merge Lines Side-by-Side**
```bash
paste file1.txt file2.txt  # Combine files column-wise
```

---

## üîÑ Modifying Text
### **`tr` - Translate or Delete Characters**
```bash
echo "hello world" | tr 'a-z' 'A-Z'  # Convert to uppercase
echo "hello 123" | tr -d '0-9'  # Remove digits
```

### **`sed` - Stream Editor for Text Substitutions**
```bash
sed 's/old/new/g' file.txt  # Replace all occurrences of 'old' with 'new'
sed '/^$/d' file.txt  # Remove empty lines
```

---

## üî¢ Counting & Hashing
### **`wc` - Count Words, Lines, Bytes**
```bash
wc file.txt  # Show lines, words, bytes
wc -l file.txt  # Count lines only
```

### **`md5sum`, `sha256sum`, `sha512sum` - Generate File Hashes**
```bash
md5sum file.txt  # Generate MD5 checksum
sha256sum file.txt  # Generate SHA-256 hash
```

---

## ‚úÖ Summary
- **Text streams** are processed using commands like `cat`, `sort`, `uniq`, `tr`, `sed`, `cut`, and more.
- **`sort` & `uniq`** help organize and filter duplicate lines.
- **`cut` & `paste`** extract and combine text columns.
- **`tr` & `sed`** modify and transform text.
- **`wc`** counts lines, words, and bytes.
- **`md5sum` & `sha256sum`** generate file integrity hashes.

---

Th